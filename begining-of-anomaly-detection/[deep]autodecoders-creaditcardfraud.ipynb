{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.6.8 (default, Apr 25 2019, 21:02:35) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]\n",
      "pandas:  0.25.1\n",
      "numpy:  1.17.2\n",
      "seaborn:  0.9.0\n",
      "matplotlib:  3.1.1\n",
      "sklearn:  0.20.4\n",
      "keras:  2.2.4\n",
      "tensorflow:  2.0.0\n",
      "(284807, 31)\n",
      "(16320, 29) train samples\n",
      "(4080, 29) test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"numpy: \", np.__version__)\n",
    "print(\"seaborn: \", sns.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn: \", sklearn.__version__)\n",
    "print(\"keras: \", keras.__version__)\n",
    "print(\"tensorflow: \", tf.__version__)\n",
    "\n",
    "# https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "filepath= './data-sample/creditcard.csv'\n",
    "df = pd.read_csv(filepath_or_buffer=filepath, header=0, sep=\",\")\n",
    "print(df.shape)\n",
    "\n",
    "# You will collect 20k normal and 400 abnormal records. \n",
    "# You can pick different ratios to try, but in general more normal data examples are better because you want to teach your autoencoder what normal data looks like. \n",
    "# Too much abnormal data in training will train the autoencoder to learn that the anomalies are actually normal, which goes against your goal.\n",
    "\n",
    "df[\"Amount\"] = StandardScaler().fit_transform(df[\"Amount\"].values.reshape(-1, 1))\n",
    "\n",
    "df0 = df.query('Class == 0').sample(20000)\n",
    "df1 = df.query('Class == 1').sample(400)\n",
    "\n",
    "df = pd.concat([df0, df1])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(labels=['Time', 'Class'], axis=1), df['Class'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 29)]              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                480       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 29)                493       \n",
      "=================================================================\n",
      "Total params: 1,329\n",
      "Trainable params: 1,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "log_file_name = \"deppautoencoder\"\n",
    "\n",
    "encoding_dim = 16\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "inputArray = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(inputArray)\n",
    "encoded = Dense(8, activation='relu')(encoded)\n",
    "encoded = Dense(4, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(8, activation='relu')(encoded)\n",
    "decoded = Dense(encoding_dim, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='softmax')(decoded)\n",
    "\n",
    "autoencoder = Model(inputArray, decoded)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 16320 samples, validate on 4080 samples\n",
      "Epoch 1/30\n",
      "16320/16320 [==============================] - 1s 82us/sample - loss: 1.4159 - mae: 0.6528 - accuracy: 0.6125 - val_loss: 1.7078 - val_mae: 0.6785 - val_accuracy: 0.6179\n",
      "Epoch 2/30\n",
      "16320/16320 [==============================] - 1s 66us/sample - loss: 1.4159 - mae: 0.6528 - accuracy: 0.6138 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6206\n",
      "Epoch 3/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4158 - mae: 0.6527 - accuracy: 0.6145 - val_loss: 1.7075 - val_mae: 0.6784 - val_accuracy: 0.6172\n",
      "Epoch 4/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4158 - mae: 0.6527 - accuracy: 0.6164 - val_loss: 1.7073 - val_mae: 0.6784 - val_accuracy: 0.6240\n",
      "Epoch 5/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4157 - mae: 0.6527 - accuracy: 0.6157 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6181\n",
      "Epoch 6/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4157 - mae: 0.6527 - accuracy: 0.6159 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6240\n",
      "Epoch 7/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4157 - mae: 0.6527 - accuracy: 0.6172 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6223\n",
      "Epoch 8/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4158 - mae: 0.6527 - accuracy: 0.6179 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6223\n",
      "Epoch 9/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4157 - mae: 0.6527 - accuracy: 0.6196 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6272\n",
      "Epoch 10/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4156 - mae: 0.6527 - accuracy: 0.6195 - val_loss: 1.7075 - val_mae: 0.6784 - val_accuracy: 0.6196\n",
      "Epoch 11/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4156 - mae: 0.6527 - accuracy: 0.6199 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6213\n",
      "Epoch 12/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4156 - mae: 0.6526 - accuracy: 0.6206 - val_loss: 1.7076 - val_mae: 0.6784 - val_accuracy: 0.6235\n",
      "Epoch 13/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4155 - mae: 0.6526 - accuracy: 0.6219 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6167\n",
      "Epoch 14/30\n",
      "16320/16320 [==============================] - 1s 66us/sample - loss: 1.4155 - mae: 0.6526 - accuracy: 0.6206 - val_loss: 1.7074 - val_mae: 0.6783 - val_accuracy: 0.6181\n",
      "Epoch 15/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4154 - mae: 0.6526 - accuracy: 0.6205 - val_loss: 1.7072 - val_mae: 0.6784 - val_accuracy: 0.6235\n",
      "Epoch 16/30\n",
      "16320/16320 [==============================] - 1s 71us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6210 - val_loss: 1.7073 - val_mae: 0.6783 - val_accuracy: 0.6184\n",
      "Epoch 17/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4154 - mae: 0.6526 - accuracy: 0.6196 - val_loss: 1.7074 - val_mae: 0.6784 - val_accuracy: 0.6272\n",
      "Epoch 18/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4154 - mae: 0.6526 - accuracy: 0.6219 - val_loss: 1.7074 - val_mae: 0.6783 - val_accuracy: 0.6228\n",
      "Epoch 19/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6218 - val_loss: 1.7073 - val_mae: 0.6783 - val_accuracy: 0.6267\n",
      "Epoch 20/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6214 - val_loss: 1.7076 - val_mae: 0.6784 - val_accuracy: 0.6105\n",
      "Epoch 21/30\n",
      "16320/16320 [==============================] - 1s 67us/sample - loss: 1.4152 - mae: 0.6526 - accuracy: 0.6214 - val_loss: 1.7072 - val_mae: 0.6783 - val_accuracy: 0.6230\n",
      "Epoch 22/30\n",
      "16320/16320 [==============================] - 1s 66us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6227 - val_loss: 1.7070 - val_mae: 0.6783 - val_accuracy: 0.6211\n",
      "Epoch 23/30\n",
      "16320/16320 [==============================] - 1s 67us/sample - loss: 1.4152 - mae: 0.6526 - accuracy: 0.6228 - val_loss: 1.7071 - val_mae: 0.6783 - val_accuracy: 0.6164\n",
      "Epoch 24/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6223 - val_loss: 1.7069 - val_mae: 0.6783 - val_accuracy: 0.6262\n",
      "Epoch 25/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4153 - mae: 0.6525 - accuracy: 0.6233 - val_loss: 1.7071 - val_mae: 0.6784 - val_accuracy: 0.6238\n",
      "Epoch 26/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4153 - mae: 0.6526 - accuracy: 0.6233 - val_loss: 1.7068 - val_mae: 0.6783 - val_accuracy: 0.6221\n",
      "Epoch 27/30\n",
      "16320/16320 [==============================] - 1s 64us/sample - loss: 1.4152 - mae: 0.6525 - accuracy: 0.6222 - val_loss: 1.7068 - val_mae: 0.6783 - val_accuracy: 0.6309\n",
      "Epoch 28/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4152 - mae: 0.6526 - accuracy: 0.6242 - val_loss: 1.7070 - val_mae: 0.6783 - val_accuracy: 0.6110\n",
      "Epoch 29/30\n",
      "16320/16320 [==============================] - 1s 65us/sample - loss: 1.4152 - mae: 0.6526 - accuracy: 0.6238 - val_loss: 1.7068 - val_mae: 0.6783 - val_accuracy: 0.6297\n",
      "Epoch 30/30\n",
      "16320/16320 [==============================] - 1s 69us/sample - loss: 1.4152 - mae: 0.6526 - accuracy: 0.6233 - val_loss: 1.7066 - val_mae: 0.6782 - val_accuracy: 0.6248\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "4080/4080 [==============================] - 0s 28us/sample - loss: 1.7066 - mae: 0.6783 - accuracy: 0.6248\n",
      "Test loss:  1.7066149784069435\n",
      "Test accuracy:  0.67825013\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['mae', 'accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train, \n",
    "                          batch_size=batch_size, \n",
    "                          epochs=epochs, \n",
    "                          verbose=1, \n",
    "                          shuffle=True, \n",
    "                          validation_data=(x_test, x_test),\n",
    "                          callbacks=[TensorBoard(log_dir='../logs/' + log_file_name)])\n",
    "\n",
    "score = autoencoder.evaluate(x_test, x_test, verbose=1)\n",
    "\n",
    "print(\"Test loss: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
